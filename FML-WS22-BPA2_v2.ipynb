{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc69b41",
   "metadata": {},
   "source": [
    "# FML - Winter Semester 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a091466-2035-4d42-981d-d3a6d9ecc808",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d902ba41b11b0b231e56b109bda1221a",
     "grade": false,
     "grade_id": "cell-be1ab0d205b915b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Bonus Points Assignment 2\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "For detailed task instructions, please refer to the assignment PDF.\n",
    "\n",
    "This assignment requires `numpy`, `matplotlib`, and `torch` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment. If the problem persists, please seek help on the forums or use [the JupyterHub profile of the lecture](https://jupyter.rwth-aachen.de/hub/spawn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99213bc-d739-4239-9420-6c4294da7cbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a9660586c49397dba1aad376605d2ac",
     "grade": false,
     "grade_id": "cell-c3d6db82e37a62a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "ROOT = Path().resolve()\n",
    "DATA = ROOT / 'data'  # This contains the path to the data / folder of the assignment\n",
    "\n",
    "CHECKS_PASSED_MESSAGE = \"Great! All checks were passed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5573b-3a48-4340-9468-543149fb678c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c486bca75da139664d17eeb44424376c",
     "grade": false,
     "grade_id": "cell-4136322d3ddf82a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0733791-0a45-4b8c-abda-fb34d4cdb666",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fab391312eef836ad334c315d9ae829",
     "grade": false,
     "grade_id": "cell-726d8968f7d013ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "All individual operations, including activation functions, will be represented as layers in this model.\n",
    "The abstract `Layer` interface defines three methods. Make yourself familiar with the `Layer` class and its methods. You don't have to implement anything here. Simply run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646cd583-e737-4310-9f1f-6dc372c44bf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd13eaa901a9d240900604db1b9be4df",
     "grade": false,
     "grade_id": "cell-99fe9032845eff40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the layer.\n",
    "        \n",
    "        Please DON'T change the names of the variables that are already defined in this method.\n",
    "        \"\"\"\n",
    "        self.input = None  # during every forward pass, we will later store the inputs to that forward pass\n",
    "        self.output = None  # during every forward pass, we will later store the outputs of that forward pass\n",
    "        self.gradient = None  # during every backward pass, we will later store the input gradient from the next layer\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input for this layer as a numpy array.\n",
    "\n",
    "        Returns:\n",
    "            The output of this layer as a numpy array.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, gradient: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a backward pass of the layer that determines the gradient.\n",
    "\n",
    "        Args:\n",
    "            gradient: Incoming gradient from the next layer as a numpy array.\n",
    "\n",
    "        Returns:\n",
    "            The gradient which will be passed to the previous layer as a numpy array.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, learn_rate: float) -> None:\n",
    "        \"\"\" Performs weight update.\n",
    "\n",
    "        Args:\n",
    "            learn_rate: Learning rate to use for the update.\n",
    "        \n",
    "        Returns:\n",
    "            Nothing. Instead, this function should update the internal state of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48616bab-02d2-4e08-9f89-b9ea4d0919d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ea7f411acbb9d44314d82e44081f002",
     "grade": false,
     "grade_id": "cell-625324a194ad177f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.1__ Implement the `MyLinearLayer` class. Leave the constructor (`__init__`) unchanged. Only modify the methods `forward`, `backward`, and `update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1788f52-3275-4ee7-9c6a-b6f4e1aa34bc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff739455d978795fa60015534bd3737",
     "grade": false,
     "grade_id": "cell-LinearLayer-from-scratch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1637498164.py, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [9]\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.w =\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        \"\"\" Initializes the linear layer with random weights.\n",
    "        \n",
    "        The weight matrix is initialized randomly.\n",
    "        The bias is initialized as 0.\n",
    "        \n",
    "        Please DON'T change the names of the variables that are already defined in this method.\n",
    "        You DON'T have to implement anything in this method.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of dimensions of the input. Inputs for the layer will have shape (batch_size, input_dim).\n",
    "            output_dim. Number of dimensions of the output. Outputs of the layer will have shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = (2 / input_dim) * np.random.randn(input_dim, output_dim)  # initialize weights with the He initializer\n",
    "        self.b = np.zeros(output_dim)  # initialize bias with zeros\n",
    "        \n",
    "        self.input = None  # during every forward pass, we will later store the inputs to that forward pass\n",
    "        self.output = None  # during every forward pass, we will later store the outputs of that forward pass\n",
    "        self.gradient = None  # during every backward pass, we will later store the input gradient from the next layer\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a forward pass of the layer.\n",
    "\n",
    "        Use self.w as the weights and self.b as the bias for the forward pass.\n",
    "        The input to this function should be stored in the internal state (in the self.input variable).\n",
    "        The output of this function should be stored in the internal state (in the self.output variable).\n",
    "\n",
    "        Args:\n",
    "            x: Input batch matrix as a numpy array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            x: Output batch matrix as a numpy array of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        x_out = np.dot(self.input,self.w) + self.b\n",
    "        self.output = x_out\n",
    "        return self.output\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, gradient: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a backward pass of the layer.\n",
    "        \n",
    "        The input gradient to this function should be stored in the internal state (in the self.gradient variable).\n",
    "\n",
    "        Args:\n",
    "            gradient: Incoming gradient from the next layer as a numpy array of shape (batch_size, output_dim).\n",
    "\n",
    "        Returns:\n",
    "            The gradient which will be passed to the previous layer as a numpy array of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        self.w = \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update(self, learn_rate: float) -> None:\n",
    "        \"\"\" Performs a weight update based on previously stored input and gradients.\n",
    "        \n",
    "        Args:\n",
    "            learn_rate: Learning rate to use for the update.\n",
    "        \n",
    "        Returns:\n",
    "            Nothing. Instead, this function should update the internal state of the layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55bce9-7d37-4e58-b6d4-b1c1b6bb2dff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64ccb4697e375e8463ebba0c7b22574a",
     "grade": false,
     "grade_id": "cell-bdebafb1dfcf699f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure ...\n",
    "- ... the output of your forward pass has the correct shape.\n",
    "- ... the gradient produced by your backward pass has the correct shape.\n",
    "- ... the parameters (weights and bias) keep their shape after being updated.\n",
    "- ... the parameters (weights and bias) are changed by your update function.\n",
    "- ... the backward pass produces the correct results for some hand-picked input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9001b48b-0b87-4e28-810c-1779b3dbdd37",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16ab0b4bd83cbb15b50778ab37a436f6",
     "grade": true,
     "grade_id": "cell-c576754be76a5311",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyLinearLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m __my_batch_size, __my_input_dim \u001b[39m=\u001b[39m __my_input\u001b[39m.\u001b[39mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m __my_output_dim \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m __my_linear_layer \u001b[39m=\u001b[39m MyLinearLayer(__my_input_dim, __my_output_dim)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m __initial_w, __initial_b \u001b[39m=\u001b[39m __my_linear_layer\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39mcopy(), __my_linear_layer\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# check if outputs of forward pass have the correct shape\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyLinearLayer' is not defined"
     ]
    }
   ],
   "source": [
    "__my_input = np.array([[1., 2., 3., 4., 5.], [1., 1., 2., 3., 5.]])\n",
    "__my_batch_size, __my_input_dim = __my_input.shape\n",
    "__my_output_dim = 16\n",
    "\n",
    "__my_linear_layer = MyLinearLayer(__my_input_dim, __my_output_dim)\n",
    "__initial_w, __initial_b = __my_linear_layer.w.copy(), __my_linear_layer.b.copy()\n",
    "\n",
    "# check if outputs of forward pass have the correct shape\n",
    "__my_output = __my_linear_layer.forward(__my_input)\n",
    "__target_output_shape = (__my_batch_size, __my_output_dim)\n",
    "assert __my_output.shape == __target_output_shape, f\"The output shape of a forward pass should have been {__target_output_shape}, but was {__my_output.shape}.\"\n",
    "assert __my_linear_layer.input is not None, \"You forgot to store the input to the forward pass.\"\n",
    "assert __my_linear_layer.output is not None, \"You forgot to store the output of the forward pass.\"\n",
    "\n",
    "\n",
    "# check if gradient computed by backward pass has the correct shape\n",
    "__my_gradient = __my_linear_layer.backward(__my_output)\n",
    "__target_gradient_shape = __my_batch_size, __my_input_dim\n",
    "assert __my_gradient.shape == __target_gradient_shape, f\"The output shape of a backward pass should have been {__target_gradient_shape}, but was {__my_gradient.shape}.\"\n",
    "assert __my_linear_layer.gradient is not None, \"Your forgot to store the input gradient to the output pass.\"\n",
    "assert np.allclose(__my_linear_layer.gradient, __my_output), \"You did not store the input gradient correctly. Did you store the output gradient instead?\"\n",
    "\n",
    "# check if updating the parameters keeps their shape and changes their values\n",
    "__my_linear_layer.update(1)\n",
    "assert __my_linear_layer.w.shape == __initial_w.shape, f\"Updating the weights of the layer should not change their shape, but shape changed from {__initial_w.shape} to {__my_linear_layer.w.shape}.\"\n",
    "assert __my_linear_layer.b.shape == __initial_b.shape, f\"Updating the bias of the layer should not change its shape, but shape changed from {__initial_b.shape} to {__my_linear_layer.b.shape}.\"\n",
    "assert np.any(__my_linear_layer.w != __initial_w), \"Updating the layer should change the weights (provided the gradient is not 0).\"\n",
    "assert np.any(__my_linear_layer.b != __initial_b), \"Updating the layer should change the bias (provided the gradient is not 0).\"\n",
    "\n",
    "# check if your backward pass produces the correct results for some arbitrary inputs\n",
    "__input = np.array([[0.10951508, 0.60428524, 0.79855139, 0.88042912]])\n",
    "__gradient = np.array([[0.45294082, 0.45302968, 0.37704032, 0.59154961]])\n",
    "\n",
    "__my_linear_layer = MyLinearLayer(4, 4)\n",
    "__my_linear_layer.w = np.array([\n",
    "    [1.28895006, -0.53177473, 0.09185581, -0.15011355],\n",
    "    [-0.42658447, 0.56021905, 0.06015589, 0.60129398],\n",
    "    [-0.20495445, 0.21709921, 0.29868546, -0.80570083],\n",
    "    [-0.20287495, 0.19254318, -1.00982607, -0.45614844]\n",
    "])\n",
    "__my_linear_layer.b = np.array([0., 0., 0., 0.])\n",
    "\n",
    "__output = __my_linear_layer.backward(__gradient.copy())\n",
    "__reference_output = np.array([[0.28874209, 0.43895475, -0.3584754, -0.65524215]])\n",
    "\n",
    "assert __output.shape == __reference_output.shape, f\"For a {__input.shape} input and an input dimensionality of {__input_dim} the output of your backward pass should be of shape {__reference_output.shape}, but was of shape {__output.shape}.\"\n",
    "assert np.allclose(__output - __reference_output, 0), f\"Your backward pass produces a wrong result for input {__input} and gradient {__gradient}.\"\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f4e6d0-3135-4c8a-9c54-278e51fa29a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ff1969500e4c8c4989e5b2e1ec33399",
     "grade": true,
     "grade_id": "cell-fe561fc8a078c306",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16cbaebf-841e-421a-b1ec-8e4c6d58e796",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "842f4b6dba1523db77ea6bfe8a6f7a19",
     "grade": true,
     "grade_id": "cell-fb7a2f6a2f0871c8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe62617b-e0c6-4653-b67c-98f45adffa3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc6e5714c728848d5a7a2caca6b3b576",
     "grade": true,
     "grade_id": "cell-476193797b894597",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d225b7b-ce6f-403c-8621-57a1fa60ceaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a28b269a1ccc3103ccdc2fddebdf55c",
     "grade": true,
     "grade_id": "cell-9c9b94c602953f79",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785318f5-5d35-406d-a1ff-3b948c0cfab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "836f7b9aac3e1a57f8dbf85c11798001",
     "grade": false,
     "grade_id": "cell-57673b5c3991a11a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.2__ Implement the `MyReLULayer` class. Only modify the methods `forward` and `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf27863e-acb8-4e94-a63a-29b6ddb98541",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d82923de9bb1caadd1d3ee1c95d00d3",
     "grade": false,
     "grade_id": "cell-ReLU-Layer-from-scratch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyReLULayer(Layer):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a forward pass of the ReLU layer.\n",
    "\n",
    "        The input to this function should be stored in the internal state (in the self.input variable).\n",
    "        The output of this function should be stored in the internal state (in the self.output variable).\n",
    "\n",
    "        Args:\n",
    "            x: Input batch matrix as a numpy array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            x: Output batch matrix as a numpy array of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, gradient: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a backward pass of the layer that determines the gradient.\n",
    "        \n",
    "        The input gradient to this function should be stored in the internal state (in the self.gradient variable).\n",
    "\n",
    "        Args:\n",
    "            gradient: Incoming gradient from the next layer as a numpy array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            The gradient which will be passed to the previous layer as a numpy array of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6e23e-f585-46b8-8019-6d8013f5ddb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f0ff3704fc66d3a23484567169e6fef",
     "grade": false,
     "grade_id": "cell-dd6ef396c7046f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure ...\n",
    "- ... the forward and backward pass produces an output of the correct shape.\n",
    "- ... the input is stored.\n",
    "- ... the output is stored.\n",
    "- ... the gradient is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c3f8f1c-a5e0-4c19-af74-268aad4632a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e30116ac384e8fe38544baba0449590",
     "grade": true,
     "grade_id": "cell-360349e648170a4f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m __my_relu_layer \u001b[39m=\u001b[39m MyReLULayer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# check if outputs of forward pass have the correct shape\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m __my_output \u001b[39m=\u001b[39m __my_relu_layer\u001b[39m.\u001b[39;49mforward(__my_input)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39massert\u001b[39;00m __my_output\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m __my_input\u001b[39m.\u001b[39mshape, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe output of a forward pass should not change the data shape, but changed from \u001b[39m\u001b[39m{\u001b[39;00m__my_input\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00m__my_output\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39massert\u001b[39;00m __my_relu_layer\u001b[39m.\u001b[39minput \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou forgot to store the input to the forward pass.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 18\u001b[0m in \u001b[0;36mMyReLULayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\" Performs a forward pass of the ReLU layer.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mThe input to this function should be stored in the internal state (in the self.input variable).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    x: Output batch matrix as a numpy array of shape (batch_size, input_dim).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "__my_input = np.array([[0, 1, 0, -1, 0], [0, 1, -1, 0.5, -0.5]])\n",
    "__my_batch_size, __my_input_dim = __my_input.shape\n",
    "\n",
    "__my_relu_layer = MyReLULayer()\n",
    "\n",
    "# check if outputs of forward pass have the correct shape\n",
    "__my_output = __my_relu_layer.forward(__my_input)\n",
    "assert __my_output.shape == __my_input.shape, f\"The output of a forward pass should not change the data shape, but changed from {__my_input.shape} to {__my_output.shape}.\"\n",
    "assert __my_relu_layer.input is not None, \"You forgot to store the input to the forward pass.\"\n",
    "assert __my_relu_layer.output is not None, \"You forgot to store the output of the forward pass.\"\n",
    "\n",
    "\n",
    "# check that the outputs are pointwise non-negative\n",
    "assert np.all(__my_output >= 0), f\"The output of a ReLU layer can't have negative values.\"\n",
    "\n",
    "\n",
    "# check if gradient computed by backward pass has the correct shape\n",
    "__my_gradient = __my_relu_layer.backward(__my_output)\n",
    "assert __my_gradient.shape == __my_output.shape, f\"The output of a backward pass should not change the gradient shape, but changed from {__my_output.shape} to {__my_gradient.shape}.\"\n",
    "assert __my_relu_layer.gradient is not None, \"Your forgot to store the gradient input to the output pass.\"\n",
    "assert np.allclose(__my_relu_layer.gradient, __my_output), \"You did not store the input gradient correctly. Did you store the output gradient instead?\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd74e802-1d72-4d01-9617-802a458ff3a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "308f342e915604a93af6e86c3861c456",
     "grade": true,
     "grade_id": "cell-0a4443b5f480a376",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "251fc494-7dbc-42b1-9e1f-c07272ca6c60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8da0a45a7859fcd1822b4a55a734617b",
     "grade": true,
     "grade_id": "cell-f4ce8db08fe8f660",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b4dea-f763-4506-8c16-c51daa9d7c65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59567b9b49c7ee0323cc9c5747be3935",
     "grade": false,
     "grade_id": "cell-8a7b6e7457e77a03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.3__ Implement the `MyNeuralNet` class. Only modify the methods `forward` and `backward`. You are not allowed to modify the constructor (`__init__`) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfad1c2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b50e417b60a2e24be9b5ec44c08996b",
     "grade": false,
     "grade_id": "cell-my-neural-net",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyNeuralNet:\n",
    "    def __init__(self, n_hidden: int):\n",
    "        self.l1 = MyLinearLayer(input_dim=1, output_dim=n_hidden)\n",
    "        self.l2 = MyLinearLayer(input_dim=n_hidden, output_dim=1)\n",
    "        self.relu = MyReLULayer()\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs a forward pass through the complete network by successively calling the forward method of its layers.\n",
    "        \n",
    "        Args:\n",
    "            x: Batched input data as a numpy array of shape (batch_size, dim_input).\n",
    "            \n",
    "        Returns:\n",
    "            The output of the network as a numpy array of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, y: np.ndarray) -> None:\n",
    "        \"\"\" Performs backward propagation by successively calling the backward method of its layers.\n",
    "        \n",
    "        Args:\n",
    "            y: The batch of target values corresponding to the last forwarded input as a numpy array of shape (batch_size, 1).\n",
    "            \n",
    "        Returns:\n",
    "            Nothing. The purpose of this method is to backpropagate the gradient through all layers of the model so that\n",
    "            they can obtain their respective gradients and later use that to update their parameters.\n",
    "        \"\"\"        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update(self, learn_rate: float) -> None:\n",
    "        \"\"\" Updates the parameters of the model by successively calling the update method of its trainable layers.\n",
    "        \n",
    "        Args:\n",
    "            learn_rate: The learning rate with which to update the parameters.\n",
    "            \n",
    "        Returns:\n",
    "            Nothing. The purpose of this method is to update the internal parameters of the model.\n",
    "        \"\"\"        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a276ccd0-f207-47d0-8ca8-2c093c131ba4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87e18a6584311eab5d0d5a021e7e1f26",
     "grade": true,
     "grade_id": "cell-c6b00c7b04c2a784",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyLinearLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 23\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m __my_neural_net \u001b[39m=\u001b[39m MyNeuralNet(\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m __batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m __predictions \u001b[39m=\u001b[39m __my_neural_net\u001b[39m.\u001b[39mforward(__batch)\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 23\u001b[0m in \u001b[0;36mMyNeuralNet.__init__\u001b[1;34m(self, n_hidden)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_hidden: \u001b[39mint\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1 \u001b[39m=\u001b[39m MyLinearLayer(input_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, output_dim\u001b[39m=\u001b[39mn_hidden)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2 \u001b[39m=\u001b[39m MyLinearLayer(input_dim\u001b[39m=\u001b[39mn_hidden, output_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu \u001b[39m=\u001b[39m MyReLULayer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyLinearLayer' is not defined"
     ]
    }
   ],
   "source": [
    "__my_neural_net = MyNeuralNet(10)\n",
    "__batch = np.random.randn(100, 1)\n",
    "__predictions = __my_neural_net.forward(__batch)\n",
    "\n",
    "__target_shape = __batch.shape\n",
    "assert __predictions.shape == __target_shape, f\"For an input batch of shape {__batch.shape}, your model should output predictions of shape {__target_shape}, but produced predictions of shape {__predictions.shape}.\"\n",
    "\n",
    "assert hasattr(__my_neural_net, \"l1\"), \"You are not allowed to change the initialisation of the model.\"\n",
    "assert hasattr(__my_neural_net, \"relu\"), \"You are not allowed to change the initialisation of the model.\"\n",
    "assert hasattr(__my_neural_net, \"l2\"), \"You are not allowed to change the initialisation of the model.\"\n",
    "assert isinstance(__my_neural_net.l1, MyLinearLayer), \"You are not allowed to change the initialisation of the model.\"\n",
    "assert isinstance(__my_neural_net.relu, MyReLULayer), \"You are not allowed to change the initialisation of the model.\"\n",
    "assert isinstance(__my_neural_net.l2, MyLinearLayer), \"You are not allowed to change the initialisation of the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e929c0-d8ab-4b3e-ae05-9690d4144a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57faa87d72f4be9e3db1e827fd0c37b2",
     "grade": true,
     "grade_id": "cell-14f8809d49061ab6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9ba766d-fa08-4d3f-9b8b-5f12ef13fda1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4008cc7188063073a8e0bf4ea3ba5b7c",
     "grade": true,
     "grade_id": "cell-2390f3c42d49c263",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6ac8180",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "505454ccf3a8fe6aa85965a75b2605b5",
     "grade": false,
     "grade_id": "cell-1a36955e917f6e65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def get_training_data_myneuralnet(batch_size: int, N: int) -> Generator[np.ndarray, None, None]:\n",
    "    \"\"\" Generates data to train MyNeuralNet.\n",
    "    \n",
    "    This function outputs a generator, i.e., something that can be used in a `for` loop after the keyword `in`.\n",
    "    The generator iterates over batches of data, and each batch is a tuple containing the input and the output.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: the batch size. Batches of inputs or outputs have shape (batch_size, 1). The only possible exception is the last batch; see below.\n",
    "        N: the total number of data points. If N % batch_size != 0, then the input and output of the last batch both have shape (N % batch_size, 1).\n",
    "    \n",
    "    Returns:\n",
    "        A generator of training data. Elements returned by the generator have the form (X, y), where X and y both have shape (b, 1), where b is the current batch's size.\n",
    "        \n",
    "    Remarks:\n",
    "        Calling this function multiple times with identical arguments will output the same data.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(1234)\n",
    "    last_batch_size = N % batch_size\n",
    "    incomplete_last_batch = last_batch_size != 0\n",
    "    n_batches = N // batch_size\n",
    "\n",
    "    def generate_data(n):\n",
    "        X = rng.uniform(-1, 1, n).reshape(-1, 1)\n",
    "        y = 2 + X * (X - 1) * (X + 1) + rng.normal(0, 0.01, size=(n,1))\n",
    "        return X, y\n",
    "\n",
    "    for _ in range(n_batches): \n",
    "        yield generate_data(batch_size)\n",
    "    if incomplete_last_batch:\n",
    "        yield generate_data(last_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bf65f-45e2-405d-9d4c-00b191dd027f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bde36a5c3f67bfbef3c2ea80a3f22ff",
     "grade": false,
     "grade_id": "cell-5e32ad7d60461fb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.4__ Implement the function `train_myneuralnet`. You only have to fill in the remaining part in the inner `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f040436",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "980f80bfe61f05edb854983c50dd7b29",
     "grade": false,
     "grade_id": "cell-train-myneuralnet",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_myneuralnet(model: MyNeuralNet, epochs: int, batch_size: int, learn_rate: float, N: int) -> list[float]:\n",
    "    \"\"\" Trains the model with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: the instance of MyNeuralNet to train\n",
    "        epochs: the number of epochs, i.e., of complete passes over the data set\n",
    "        batch_size: the batch size to use in minibatch gradient descent\n",
    "        learn_rate: the learning rate to use in minibatch gradient descent\n",
    "        N: the number of data points in the training set. The training data is generated by the function get_training_data_myneuralnet.\n",
    "    \n",
    "    Returns:\n",
    "        losses: the list of losses at the end of each epoch\n",
    "    \"\"\"\n",
    "    # We store the losses for plotting later\n",
    "    losses = []\n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            # This variable cumulates the loss until the current batch\n",
    "            running_loss = 0.\n",
    "            for X_batch, y_batch in get_training_data_myneuralnet(batch_size, N):\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "            # At the end of the epoch, we divide running_loss by the number of samples and get the average loss\n",
    "            losses.append(running_loss / N)\n",
    "            pbar.set_description(f\"Loss {losses[-1]:.05f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a3d80-3e55-4096-b330-7e866fd0d434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fd4df28d25c989c8eb1a6ffdd5646fc",
     "grade": false,
     "grade_id": "cell-453e035b65bf868a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.5__ Train the neural network. Here you simply have to run the following cell and check that the resulting plot looks similar to the one in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d0505db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a50fce59340c887e598e2736668387",
     "grade": false,
     "grade_id": "cell-e033619d4e82f563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyLinearLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 30\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ax2\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mLearning curve\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m train_and_plot_myneuralnet(n_hidden\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, learn_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, N\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 30\u001b[0m in \u001b[0;36mtrain_and_plot_myneuralnet\u001b[1;34m(n_hidden, epochs, batch_size, learn_rate, N)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_plot_myneuralnet\u001b[39m(n_hidden: \u001b[39mint\u001b[39m, epochs: \u001b[39mint\u001b[39m, batch_size: \u001b[39mint\u001b[39m, learn_rate: \u001b[39mfloat\u001b[39m, N: \u001b[39mint\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\" Trains a MyNeuralNet with the specified parameters, and plots the learned function and the training curve\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m        N: the number of data points in the training set. The training data is generated by the function get_training_data_myneuralnet.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model \u001b[39m=\u001b[39m MyNeuralNet(n_hidden\u001b[39m=\u001b[39;49mn_hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     losses \u001b[39m=\u001b[39m train_myneuralnet(model, epochs\u001b[39m=\u001b[39mepochs, batch_size\u001b[39m=\u001b[39mbatch_size, learn_rate\u001b[39m=\u001b[39mlearn_rate, N\u001b[39m=\u001b[39mN)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     X_train, y_train \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(get_training_data_myneuralnet(N, N))  \u001b[39m# load the whole data set\u001b[39;00m\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 30\u001b[0m in \u001b[0;36mMyNeuralNet.__init__\u001b[1;34m(self, n_hidden)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_hidden: \u001b[39mint\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1 \u001b[39m=\u001b[39m MyLinearLayer(input_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, output_dim\u001b[39m=\u001b[39mn_hidden)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2 \u001b[39m=\u001b[39m MyLinearLayer(input_dim\u001b[39m=\u001b[39mn_hidden, output_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu \u001b[39m=\u001b[39m MyReLULayer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyLinearLayer' is not defined"
     ]
    }
   ],
   "source": [
    "def train_and_plot_myneuralnet(n_hidden: int, epochs: int, batch_size: int, learn_rate: float, N: int):\n",
    "    \"\"\" Trains a MyNeuralNet with the specified parameters, and plots the learned function and the training curve\n",
    "    \n",
    "    Args:\n",
    "        n_hidden: number of hidden neurons of the model\n",
    "        epochs: the number of epochs, i.e., of complete passes over the data set\n",
    "        batch_size: the batch size to use in minibatch gradient descent\n",
    "        learn_rate: the learning rate to use in minibatch gradient descent\n",
    "        N: the number of data points in the training set. The training data is generated by the function get_training_data_myneuralnet.\n",
    "    \"\"\"\n",
    "    model = MyNeuralNet(n_hidden=n_hidden)\n",
    "    losses = train_myneuralnet(model, epochs=epochs, batch_size=batch_size, learn_rate=learn_rate, N=N)\n",
    "    X_train, y_train = next(get_training_data_myneuralnet(N, N))  # load the whole data set\n",
    "    X_train = X_train.flatten()\n",
    "    y_train = y_train.flatten()\n",
    "    X_plot = np.linspace(-1, 1)\n",
    "    y_plot = model.forward(X_plot.reshape(-1, 1)).flatten()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n",
    "    ax1.scatter(X_train, y_train, label='Training data')\n",
    "    ax1.plot(X_plot, y_plot, label='Learned function', color='red')\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_title('Training outcome')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax2.semilogy(np.arange(epochs)+1, losses)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Learning curve')\n",
    "    plt.show()\n",
    "\n",
    "train_and_plot_myneuralnet(n_hidden=200, epochs=1500, batch_size=4, learn_rate=0.01, N=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d1403-2e40-4ce8-ae73-f280f459aeff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77053b126ec91c45c45095fdfd93929d",
     "grade": false,
     "grade_id": "cell-ffce465420cd2dff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48df160-3fdd-4019-84d7-38812f24dfaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ead6a0776479fbf516b47a8b63bf9715",
     "grade": false,
     "grade_id": "cell-53f36d89d40ba0f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.6__ Implement the function `add_padding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "966a31b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67c1c875c4f5ed3f6ea6f1ed4eb7600f",
     "grade": false,
     "grade_id": "cell-add-padding",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_padding(image: np.ndarray, P: int) -> np.ndarray:\n",
    "    \"\"\" Adds padding to a batch of images.\n",
    "        \n",
    "    The padding valu should be 0. Padding images should not change the number of channels or the batch size.\n",
    "    \n",
    "    Args:\n",
    "        image: Batch of images as a numpy array of shape (batch_size, height, width, n_channels).\n",
    "        P: Number of zeros to add at all four sides of each image.\n",
    "        \n",
    "    Returns:\n",
    "        The padded image as a numpy array of shape (batch_size, height + 2P, width + 2P, n_channels).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421e2d9-b1ca-4ea5-81d6-8dcb06ba0221",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c314b17cb8130522c873d048b14607ab",
     "grade": false,
     "grade_id": "cell-4ec07719533585cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure your implementation outputs the correct shape, uses the correct padding value of 0, and works correctly if `P=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc90caf2-4a28-4aa2-adc6-2fdcdad9a14b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60caa778b5c67eecc24afe82e15706bd",
     "grade": true,
     "grade_id": "cell-229d6b9ce4a22274",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 35\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m __padding \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m __images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(__batch_size, __height, __width, __n_channels)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m __padded_images \u001b[39m=\u001b[39m add_padding(__images, __padding)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m __target_shape \u001b[39m=\u001b[39m (__batch_size, __height \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m __padding, __width \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m __padding, __n_channels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39massert\u001b[39;00m __padded_images\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m __target_shape, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor a batch of \u001b[39m\u001b[39m{\u001b[39;00m__batch_size\u001b[39m}\u001b[39;00m\u001b[39m images of height \u001b[39m\u001b[39m{\u001b[39;00m__height\u001b[39m}\u001b[39;00m\u001b[39m and width \u001b[39m\u001b[39m{\u001b[39;00m__width\u001b[39m}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{\u001b[39;00m__n_channels\u001b[39m}\u001b[39;00m\u001b[39m channels, the padded image batch should have shape \u001b[39m\u001b[39m{\u001b[39;00m__target_shape\u001b[39m}\u001b[39;00m\u001b[39m, but had shape \u001b[39m\u001b[39m{\u001b[39;00m__padded_images\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 35\u001b[0m in \u001b[0;36madd_padding\u001b[1;34m(image, P)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\" Adds padding to a batch of images.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mThe padding valu should be 0. Padding images should not change the number of channels or the batch size.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    The padded image as a numpy array of shape (batch_size, height + 2P, width + 2P, n_channels).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "__batch_size = 5\n",
    "__height = 16\n",
    "__width = 26\n",
    "__n_channels = 3\n",
    "__padding = 2\n",
    "__images = np.random.randn(__batch_size, __height, __width, __n_channels)\n",
    "__padded_images = add_padding(__images, __padding)\n",
    "\n",
    "__target_shape = (__batch_size, __height + 2 * __padding, __width + 2 * __padding, __n_channels)\n",
    "assert __padded_images.shape == __target_shape, f\"For a batch of {__batch_size} images of height {__height} and width {__width} with {__n_channels} channels, the padded image batch should have shape {__target_shape}, but had shape {__padded_images.shape}\"\n",
    "\n",
    "__padded_images[:, __padding:-__padding, __padding:-__padding, :] = 0\n",
    "assert np.allclose(__padded_images, 0), \"The values added by the padding should all be 0.\"\n",
    "\n",
    "assert np.all(__images == add_padding(__images, 0)), \"If P=0, the padding function should not change the input batch, but your implementation did.\"\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4afa5a98-1f18-4211-b57c-1701758b08cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "435678d2898c47f0b0adaff1cd859611",
     "grade": true,
     "grade_id": "cell-688cd86b8eb4a4ee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb6245-2cda-464d-add6-3e492971bb6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72793595853fc2d974f0ae4d46192e60",
     "grade": false,
     "grade_id": "cell-02f4a19e69c35ef4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.7__ Implement the function `convolve`. We have already given you some code to start with. Particularly, `F_out` should be the number of channels your convolution outputs, `h_out` should be the height of the images your convolution outputs, and `w_out` should be the width of the images that your convolution outputs. You only have to fill in the part of the code that performs the actual convolution of the input image batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52b6907a-e5d3-4404-8d4e-d5a945bfd251",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3401de7c8a6ceae3a360d687f5ea6140",
     "grade": false,
     "grade_id": "cell-convolve",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convolve(kernels: np.ndarray, images: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\" Computes the convolution of a given batch of images with given kernels and a given stride.\n",
    "    \n",
    "    You should assume that the given images are already properly padded.\n",
    "    \n",
    "    Args:\n",
    "        kernels: The kernels as a numpy array of shape (K, K, F_in, F_out).\n",
    "        images: The batch matrix containing the images as a numpy array of shape (batch_size, h_in, w_in, F_in).\n",
    "        stride: The positive stride.\n",
    "    \n",
    "    Returns:\n",
    "        The convolved batch of images as a numpy array of shape (batch_size, h_out, w_out, F_out).\n",
    "    \"\"\"\n",
    "    batch_size, h_in, w_in, F_in = images.shape  # extract shape information\n",
    "    F_out = kernels.shape[3]  # number of output channels of the kernels\n",
    "    K = kernels.shape[0]  # size of the kernels\n",
    "    \n",
    "    # make sure the number of input channels matches\n",
    "    assert F_in == kernels.shape[2], f\"Size mismatch: images and kernels have different number of input filters ({F_in} != {kernels.shape[2]}).\"\n",
    "    \n",
    "    # make sure every kernel is square\n",
    "    assert kernels.shape[0] == kernels.shape[1], \"Every kernel must be a square matrix.\"\n",
    "    \n",
    "    h_out = (h_in - K) / stride + 1  # target height of the convolved output\n",
    "    w_out = (w_in - K) / stride + 1  # target width of the convolved output\n",
    "    \n",
    "    # make sure that height and width are integers (i.e., that the kernels \"fit\" the image in size)\n",
    "    assert int(h_out) == h_out and int(w_out) == w_out, \"Incompatible dimensions.\"\n",
    "    h_out = int(h_out)  # cast height from float to int\n",
    "    w_out = int(w_out)  # cast width from float to int\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e381cd7-872e-40c8-9af6-735aded876f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bedb95e7e7885351bb06840c486c4de",
     "grade": false,
     "grade_id": "cell-a3454f198d1b6777",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure that your convolution produces the correct shape on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8109f1ff-ede6-42aa-b5cc-21a296a78b17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfc5b9cc8aba1ca9d77edc344b8c6e0",
     "grade": true,
     "grade_id": "cell-17b06cbe3f0fbfd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 40\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m __images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m2\u001b[39m, __h_in, __w_in, \u001b[39m3\u001b[39m)  \u001b[39m# 2 images of size 4x6 with 3 filters\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m __batch_size \u001b[39m=\u001b[39m __images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m __output \u001b[39m=\u001b[39m convolve(__kernels, __images, __stride)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m __target_shape \u001b[39m=\u001b[39m (__batch_size, (__h_in \u001b[39m-\u001b[39m __kernel_size) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m __stride \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, (__w_in \u001b[39m-\u001b[39m __kernel_size) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m __stride \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, __f_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39massert\u001b[39;00m __output\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m __target_shape, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe output of your convolution should be of shape \u001b[39m\u001b[39m{\u001b[39;00m__target_shape\u001b[39m}\u001b[39;00m\u001b[39m for a batch of \u001b[39m\u001b[39m{\u001b[39;00m__batch_size\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m__images\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m images and a stride of \u001b[39m\u001b[39m{\u001b[39;00m__stride\u001b[39m}\u001b[39;00m\u001b[39m, but was \u001b[39m\u001b[39m{\u001b[39;00m__output\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 40\u001b[0m in \u001b[0;36mconvolve\u001b[1;34m(kernels, images, stride)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m w_out \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(w_out)  \u001b[39m# cast width from float to int\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X54sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "__kernel_size = 2\n",
    "__f_out = 4\n",
    "__stride = 2\n",
    "__h_in = 4\n",
    "__w_in = 6\n",
    "__kernels = np.linspace(0, 1, __kernel_size * __kernel_size * 3 * __f_out).reshape(__kernel_size, __kernel_size, 3, __f_out)  # 4 kernels of shape (2, 2, 3)\n",
    "__images = np.linspace(0, 10, 2 * 4 * 6 * 3).reshape(2, __h_in, __w_in, 3)  # 2 images of size 4x6 with 3 filters\n",
    "__batch_size = __images.shape[0]\n",
    "__output = convolve(__kernels, __images, __stride)\n",
    "\n",
    "__target_shape = (__batch_size, (__h_in - __kernel_size) // __stride + 1, (__w_in - __kernel_size) // __stride + 1, __f_out)\n",
    "assert __output.shape == __target_shape, f\"The output of your convolution should be of shape {__target_shape} for a batch of {__batch_size} {__images.shape[1:]} images and a stride of {__stride}, but was {__output.shape}.\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522f25d-8f66-4320-ab61-1875d74558cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6250476f89137197fa12aa65affe6c20",
     "grade": false,
     "grade_id": "cell-8ba81b67f20f4a13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 1.8__ Perform sharpening, edge detection, and blurring of the given example image. Here you simply have to run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6797fd2-7222-4ff2-a79e-6a5e0d4bda33",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7837d57f06cce362b640920076aba09",
     "grade": true,
     "grade_id": "cell-ac7d095d625bd602",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 42\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3e58ed-8cc0-4950-a81b-d1f535dc9cc5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d0cf2fa12c697f42c45cdf29e0706a6",
     "grade": true,
     "grade_id": "cell-b696ce128a844ef9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f30ca-2983-4e3e-baea-9b60512efd4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90768c84e801c073e80923e3dd2c09bb",
     "grade": false,
     "grade_id": "cell-d12829e27211a3b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Convolutions in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6bbe66-1cad-4f7c-b2b9-6a78d3119035",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c30eb63d5ff0e0ca354f5bed93794d2",
     "grade": false,
     "grade_id": "cell-4e7b2fac6d9def87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import Tensor\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ce466-ac73-49f8-b316-3060f8cbb5b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f64ae64cd187641a359d451dff5a1aeb",
     "grade": false,
     "grade_id": "cell-698f67b2dcda83df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to load the data from the `DATA` directory. Here you don't have to implement anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a1fe3f4-8727-4bb6-8ba6-c08fba908822",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14c1e824f074dfdac23957b59f13b2d",
     "grade": false,
     "grade_id": "cell-30bd01d54e3cc3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_valid_ds = ImageFolder(\n",
    "    root=str(DATA/\"train\"),\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_valid_size = len(train_valid_ds)\n",
    "train_size = int(0.7 * train_valid_size)\n",
    "valid_size = train_valid_size - train_size\n",
    "\n",
    "train_ds, valid_ds = random_split(train_valid_ds, [train_size, valid_size])\n",
    "\n",
    "test_ds = ImageFolder(\n",
    "    root=str(DATA/\"test\"),\n",
    "    transform=transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887efa1-aabc-4fa1-9c7b-d18cd67c28bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65a69d04898ee1258dabe47b8270dafd",
     "grade": false,
     "grade_id": "cell-36e5447fd8d289cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 2.1__ Implement the `SimpLeNet` Neural Network as stated in the assignment PDF. In the `__init__` method, you should initialise all the `torch` modules that you need for the model. The `forward` method should perform a forward pass through the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17d72d68-e858-4572-b701-979980e0cfea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30bf6d0018db7f337afa856cb984ec45",
     "grade": false,
     "grade_id": "cell-SimpLeNet",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, ReLU, Sigmoid\n",
    "\n",
    "\n",
    "class SimpLeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialized the neural network.\n",
    "        \n",
    "        Here, you define all the necessary layers that are subsequently used in the forward pass.\n",
    "        Hint: Take a look at the imports above.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(3,6,3,4,1)\n",
    "        self.pool = torch.nn.MaxPool2d(4,2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.ff1 = torch.nn.Linear(6*27*27, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ff2 = torch.nn.Linear(10,1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "       \n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\" Performs a forward pass through the entire network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input batch matrix as a pytorch tensor of shape (batch_size, 224, 224, 3).\n",
    "            \n",
    "        Returns:\n",
    "            The output as a pytorch tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        x = self.pool(self.conv(x))\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.relu(self.ff1(x))\n",
    "        x = self.sigmoid(self.ff2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad304fe9-b618-41a8-9563-08e5adf7aae3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b985225badd54b1e23abb6b45dde31f1",
     "grade": false,
     "grade_id": "cell-97f4632848e52f32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure a forward pass through your model produces the correct output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b15f1e7b-2151-4604-a503-dc3471366517",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eac7078beec1a027abd9043d83aeaa84",
     "grade": true,
     "grade_id": "cell-ec21cd51d262c2ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great! All checks were passed.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__sample, _ = train_ds[0]\n",
    "__my_neural_network = SimpLeNet()\n",
    "__sample_batch = __sample.unsqueeze(0)\n",
    "\n",
    "__output =__my_neural_network(__sample_batch)\n",
    "__target_output_shape = (1, 1)\n",
    "assert __output.shape == __target_output_shape, f\"Your neural network receives an input of shape {tuple(__sample_batch.shape)} and should return an output of shape {__target_output_shape}, but output was of shape {tuple(__output.shape)}.\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "670fe401-dcae-4188-b6c2-34e08f25c000",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50d6b1bd98da34289a4fcd6a54038cd5",
     "grade": true,
     "grade_id": "cell-b058f51a746b055d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda108b4-5f8f-4d75-acba-be9fd5ff8c20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66f46b4e4a5ec93ef9f80914f38553c4",
     "grade": false,
     "grade_id": "cell-2c53a171ecc02f95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 2.2__ Implement the function `train` that trains a given `model` for a given number of `epochs` using a given `batch_size` and `learning_rate`. We already implemented part of the code for you. You should use the `criterion` to compute the loss of a batch of predictions and targets. You should use the `optimizer` to perform the actual gradient descent. The `train_loader` already holds all input batches. The missing part that you have to implement is to make predictions for the `inputs`, calculate the loss given these predictions and the `labels`, backpropagate the loss, and perform an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "26d9189c-cfff-4445-b913-b176993fa1ba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4774b9c47b7c1995f555aff835a7b0b",
     "grade": false,
     "grade_id": "cell-train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(model, batch_size, epochs, learning_rate):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            samples_in_epoch = 0\n",
    "            \n",
    "            with tqdm(train_loader) as ebar:\n",
    "                for inputs, labels in ebar:\n",
    "                    optimizer.zero_grad()  # reset the parameter gradients to 0\n",
    "                    labels = labels.float()  # cast label type to float\n",
    "                    # forward + backward + optimize\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs,labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "            \n",
    "                    running_loss += loss.item() * inputs.shape[0]  # keep track of current loss\n",
    "                    samples_in_epoch += inputs.shape[0]\n",
    "                    ebar.set_description(f\"Epoch {epoch+1} | Loss {running_loss/samples_in_epoch:0.04f}\")\n",
    "                losses.append(running_loss / len(train_ds))  # compute and store average loss during the epoch\n",
    "                \n",
    "            pbar.set_description(f\"Total Training | Loss {losses[-1]:.04f}\")  # show loss during the last epoch\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6b60a-a9a6-4054-90a0-da8acf884f1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fd8cd72001196b6ea82ef9d8f6d4768",
     "grade": false,
     "grade_id": "cell-752317a3487f81d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 2.3__ Run the following cell to train your model. Note that this might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bd1a9d63-150c-4448-ad95-3b041de633d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ff31a117d52e6f7e8b9fc84801f79df",
     "grade": true,
     "grade_id": "cell-67581d9b33f332a5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1523b464f9cb4867ae1d31fa8d46efca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7659da7ee6e34861b73bc667d0b0d85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f5b97d1ceb4697aa28ea076f5ea141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2548b85614be4fdd82f10a28100e9114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816f4df34f3740a79b9a63695549f5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652cc170b7df416d84a08faf3d0b3879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__model = SimpLeNet()\n",
    "__epochs = 5\n",
    "__learning_rate = 0.01\n",
    "__batch_size = 128\n",
    "losses = train(__model, __batch_size, __epochs, __learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0900d3-16bc-412b-a2aa-dca791a89b6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8670ef9467671a95b2a2201c4e589769",
     "grade": false,
     "grade_id": "cell-59ccc4caeae8ed80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to evaluate the accuracy of the model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e445793d-8407-4845-95b1-a7b6627e186f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ccc0559bf62d0c7868f5ca1beffad7",
     "grade": false,
     "grade_id": "cell-e7f723e78a69a2f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model achieved an accuracy of 80.40% on the validation dataset.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model)-> float:\n",
    "    validation_loader = DataLoader(valid_ds)\n",
    "    correct_classifications = 0\n",
    "    \n",
    "    for inputs, labels in validation_loader:\n",
    "        labels = labels.float()  # cast label type to float\n",
    "        predictions = model(inputs).squeeze(-1) >= 0.5  # map probabilities to binary classifications\n",
    "        correct_classifications += torch.sum(predictions.int() == labels)  # count number of correct classifications\n",
    "\n",
    "    return correct_classifications / len(valid_ds)\n",
    "        \n",
    "accuracy = evaluate(__model)\n",
    "print(f\"The model achieved an accuracy of {100 * accuracy:.2f}% on the validation dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5c140-3efa-4a25-b427-b721acca403d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e992d81f9cde216abe861efdb1303666",
     "grade": false,
     "grade_id": "cell-93bcdea68af4e4b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Extending Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c40783-b336-44be-b9c4-3d3ddebd051e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76ebf1efe5029e3a5c992af90ce2c2e7",
     "grade": false,
     "grade_id": "cell-83548074688ff4af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 3.1__ Implement the `AdaptedVGG` Neural Network. In the constructor (`__init__`), make sure that only gradients of the `classifier` are tracked by PyTorch and that gradients of the VGG module are not being tracked. Additionally, implement the `forward` function of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "968a9746-c3cd-4e28-a474-e19035c1d80b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee92186c101dbdefbf92b4c32af2c54",
     "grade": false,
     "grade_id": "cell-748651b5649f1700",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptedVGG(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the module.\n",
    "        \n",
    "        We load the VGG module, select the layers that we are interested in and set VGG to evaluation mode as we \n",
    "        don't plan on training the VGG part of the model.\n",
    "        Additionally, we define a custom classifier that works with the representations obtained fro the VGG model.\n",
    "        \n",
    "        Here, your task is simply to make sure that PyTorch doesn't track the gradients of the VGG model as this adds\n",
    "        a lot of computational overhead which is not necessary as we don't want to train the VGG model anyway. For this,\n",
    "        each PyTorch module (incldung the VGG model) already implements th function requires_grad_(...) which you can use\n",
    "        to prevent PyTorch from tracking gradients for specific modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vgg = torch.hub.load(\"pytorch/vision:v0.10.0\", \"vgg11\", weights=\"VGG11_Weights.DEFAULT\")  # load pretrained VGG model\n",
    "        del self.vgg.classifier[5:7]  # delete final classifying layers from VGG model\n",
    "        self.vgg.eval()  # deactivate the dropout layers of VGG\n",
    "\n",
    "        # add a custom classification step\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4096, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Do not change what precedes\n",
    "\n",
    "\n",
    "    \n",
    "        self.vgg = self.vgg.requires_grad_(False)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\" Performs a forward pass through the neural network.\n",
    "        \n",
    "        First, use the VGG network to obtain an expressive latent representation of the image.\n",
    "        Then, use the custom classifier to make an actual prediction about the input.\n",
    "        \n",
    "        Args:\n",
    "            x: Batch of input images as a pytorch tensor of shape (batch_size, 3, 224, 224).\n",
    "            \n",
    "        Returns:\n",
    "            A batch of predictions as a pytorch tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.classifier(self.vgg(x))\n",
    "        return x\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        \"\"\" Sets the module in training mode and ensures that VGG remains in eval mode.\n",
    "        \"\"\"\n",
    "        super().train(mode)\n",
    "        self.vgg.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23798ffc-a9ff-43bc-916c-009a4cc5cafc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58e4c642b185589f324192375a7a1d4c",
     "grade": false,
     "grade_id": "cell-6a3582ad27dff56c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure you output the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e596bf4-f527-42d9-9852-8767d5a8ba26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc44a4390a2c393b2711e5969e5a410e",
     "grade": true,
     "grade_id": "cell-af6595fd9086a9d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sagar/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Great! All checks were passed.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__model = AdaptedVGG()\n",
    "__parameters = __model.parameters()\n",
    "__n_parameters = sum(parameter.numel() for parameter in __parameters if parameter.requires_grad)\n",
    "__target_n_parameters = 65569\n",
    "assert __n_parameters == __target_n_parameters, f\"The classifier has {__target_n_parameters} parameters, but your model has {__n_parameters} trainable parameters. Hint: Use the .requires_grad_(False) method of the VGG module to stop tracking gradients of the VGG module.\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acdf3c-77db-4600-a470-222ce1027e39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24cfc90d349e6261d6341f0603a5f91d",
     "grade": false,
     "grade_id": "cell-e97ad4a307b209fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to make sure the output of the network has the expected properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7008a22e-0a03-47c3-a530-4401a9d255d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "412e1b36b4b5b31187a2267551630dd3",
     "grade": true,
     "grade_id": "cell-dda5b8ae3eb209c0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sagar/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Great! All checks were passed.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__model = AdaptedVGG()\n",
    "__batch_size = 16\n",
    "__random_batch = torch.rand(__batch_size, 3, IMG_SIZE, IMG_SIZE)\n",
    "__predictions = __model(__random_batch)\n",
    "\n",
    "__target_shape = (__batch_size, 1)\n",
    "assert __predictions.shape == __target_shape, f\"The output of your model should have shape {__target_shape}, but your returned a tensor of shape {tuple(__predictions.shape)}.\"\n",
    "assert torch.all(__predictions >= 0), \"The output of your model should be probabilities, but you returned values smaller than 0.\"\n",
    "assert torch.all(__predictions <= 1), \"The output of your model should be probabilities, but you returned values bigger than 1.\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e80d395-4316-438b-88eb-b210bcad84c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738bdbcb8d0ab47cd4c77c1f080ddad8",
     "grade": true,
     "grade_id": "cell-4ad911bf23b9f626",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2745e-4063-4e86-83fc-bc552c241dc5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6a008ef2a5d76516e8f9d733b89996f",
     "grade": false,
     "grade_id": "cell-423585a2f2010385",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 3.2__ Implement the function `early_stopping_check` that decided whether to stop the training based on the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aede68c8-2293-45b7-85e5-a01a25bd9f42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35ecbd31567b7802cf6b04c029d45fd4",
     "grade": false,
     "grade_id": "cell-early-stopping-check",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def early_stopping_check(losses: list[float], threshold: int) -> bool:\n",
    "    \"\"\" Checks whether to stop early during training.\n",
    "    The check should decide to stop early if and only if a certain theshold of losses have been monotonoically increasing.\n",
    "    \n",
    "    Args:\n",
    "        losses: List of scalar losses for each epoch. For each epoch there is exactly one loss. losses[0] corresponds to the first epoch. losses[-1] corresponds to the last epoch.\n",
    "        threshold: Number of latest losses to be monotonically increasing for the training to stop early.\n",
    "        \n",
    "    Returns:\n",
    "        True if the training should be stopped and otherwise False.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(losses)-1):\n",
    "        if losses[i+1] > losses[i]:\n",
    "            count += 1 \n",
    "        else:\n",
    "            count = 0\n",
    "        if count == threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4cdc6d8-cec2-47fa-b0c2-a007bf3dfd92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a4ee06a8e0271ced6e88e452d4cb157",
     "grade": true,
     "grade_id": "cell-62bd41a59548f01d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5a828-b4d0-4357-ac22-1c3f214cbca8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc1b344a51d0f1dfe237ef0fb0e2b56a",
     "grade": false,
     "grade_id": "cell-36c705af3830a381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 3.3__ Implement the `forward` pass of the `DropoutAdaptedVGG` Neural Network. The model should behave just like the `AdaptedVGG`, but should randomly set $20\\%$ of the representation between the VGG output and the custom classifier to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51f5e5bf-cec2-4618-a028-fb9073c31d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8716dde91a43da0c8cadcede9d37fee2",
     "grade": false,
     "grade_id": "cell-dropout-adapted-vgg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import dropout\n",
    "\n",
    "\n",
    "class DropoutAdaptedVGG(AdaptedVGG):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f774fd3-1e6e-423d-b606-d3ad1fac760e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bab897d6937919f2e86ac10c8c78018f",
     "grade": true,
     "grade_id": "cell-982f4d5e7072d569",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sagar/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Great! All checks were passed.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__model = DropoutAdaptedVGG()\n",
    "assert isinstance(__model, AdaptedVGG), \"Your model should extend the `AdaptedVGG` model. You are not allowed to change the class signature.\"\n",
    "assert hasattr(__model, \"vgg\"), \"Your model should contain a module called `vgg`. You are not allowed to change attribute names.\"\n",
    "assert hasattr(__model, \"classifier\"), \"Your model should contain a module called `vgg`. You are not allowed to change attribute names.\"\n",
    "\n",
    "\n",
    "CHECKS_PASSED_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59e23840-d3cf-4c0e-9736-ab74e5109b7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a66f33c0083ccf0a238b063cad4737f",
     "grade": true,
     "grade_id": "cell-7709d246b8d42be8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a71a2f-2e86-4c2d-beec-a8ea900a4164",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a26ffc8bda64efaf3aaa479fba4f80",
     "grade": false,
     "grade_id": "cell-71ef7538d2b2965a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 3.4__ Implement the function `store_checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ae0807c6-ee47-4980-bfd5-3f2bf496b992",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d5c0411bd253ce5aa17ccd8e866b29",
     "grade": false,
     "grade_id": "cell-store-checkpoint",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = ROOT / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=False)\n",
    "\n",
    "def store_checkpoint(model: torch.nn.Module, epoch: int) -> None:\n",
    "    \"\"\" Saves the state of a model to disk.\n",
    "    \n",
    "    Specifically, the model should be saved in a .pt file in the CHECKPOINT_DIR folder.\n",
    "    The file name of the model should be `model-{epoch}`.\n",
    "    \n",
    "    Args:\n",
    "        model: Module to be saved to disk.\n",
    "        epoch: Epoch at which the model was saved.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    for i in range(epoch): \n",
    "        torch.save(model,\"CHECKPOINT_DIR\\model-{i}.pt\")\n",
    "        #check_point_files = open(f\"CHECKPOINT_DIR\\model-{i}.pt\", \"w\")\n",
    "        # run the model till i-th epoch and write it in the .pt file\n",
    "        #check_point_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa5b7fd6-1483-49b1-b5c6-52fa7c8512e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26e96b4cc63c610d66204d346909922b",
     "grade": true,
     "grade_id": "cell-2c7d2fd5c6566776",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is used for grading. Do not remove. Do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcd265-9f9e-48aa-95c9-438614784827",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8aec4d7734a7e64366af112f60d10c6",
     "grade": false,
     "grade_id": "cell-1353c2495147bec9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Question 3.5__ Implement the remaining part of the training loop just as you did in Question 2.2. Here you just have to fill in the missing part in the inner `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a4395db-f1ec-49e2-ac3a-59c4b4861877",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cc9914f44c4725d6701101816bd7932",
     "grade": false,
     "grade_id": "cell-51e6dea77048bc6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def transfer_train(model: torch.nn.Module, batch_size: int, epochs: int, learning_rate: float) -> list[float]:\n",
    "    assert \"store_checkpoint\" in globals(), \"The store_checkpoint function is not defined.\"\n",
    "    assert \"early_stopping_check\" in globals(), \"The early_stopping_check function is not defined.\"\n",
    "    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=batch_size)\n",
    "    \n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    CHECKPOINT_DIR.mkdir(exist_ok=True, parents=False)\n",
    "    store_checkpoint(model, 0)\n",
    "    \n",
    "    with tqdm(range(1, epochs + 1)) as pbar:\n",
    "        for epoch in pbar:  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            samples_in_epoch = 0\n",
    "            with tqdm(train_loader) as ebar:\n",
    "                for i, (inputs, labels) in enumerate(ebar):\n",
    "                    optimizer.zero_grad()  # reset the parameter gradients to 0\n",
    "                    labels = labels.float()  # cast label type to float\n",
    "                    # forward + backward + optimize\n",
    "                    \n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    predictions = model(inputs)\n",
    "                    loss = criterion(predictions, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.shape[0]  # keep track of current loss\n",
    "                    samples_in_epoch += inputs.shape[0]\n",
    "                    ebar.set_description(f\"Epoch {epoch} | Loss {running_loss/samples_in_epoch:0.04f}\")\n",
    "                \n",
    "            losses.append(running_loss / len(train_ds))\n",
    "            pbar.set_description(f\"Total training | Loss {losses[-1]:.02f}\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            samples_in_epoch = 0\n",
    "                \n",
    "            with tqdm(valid_loader) as ebar:\n",
    "                for inputs, labels in ebar:\n",
    "                    labels = labels.float()  # cast label type to float\n",
    "                    predictions = model(inputs).squeeze(-1)\n",
    "                    loss = criterion(predictions, labels)\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.shape[0]  # keep track of current loss\n",
    "                    samples_in_epoch += inputs.shape[0]\n",
    "                    ebar.set_description(f\"Epoch {epoch} | Validation Loss {running_loss/samples_in_epoch:0.04f}\")\n",
    "            \n",
    "            valid_losses.append(running_loss / len(valid_ds))\n",
    "            \n",
    "            store_checkpoint(model, epoch)\n",
    "            \n",
    "            if early_stopping_check(valid_losses, 3):\n",
    "                break\n",
    "    return losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae573480-3aa8-4ac3-855d-0717224d9b9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b584837ec28ffd554a83d0bfa5e03cc5",
     "grade": false,
     "grade_id": "cell-970dceec223fea16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to fine-tune your model. Note that this might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e05f5f61-fa6c-4563-8602-efe88d34be7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac54da966a8f76953c053861a57562e1",
     "grade": true,
     "grade_id": "cell-891aad823d7b1412",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sagar/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40456cbb3dd482a89b5d73b100a93ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d44fb29a4164579b24f0ee652f19d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 3, 224, 224])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 80\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m __learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m __model\u001b[39m.\u001b[39mtrain()  \u001b[39m# set model in training mode\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m losses, valid_losses \u001b[39m=\u001b[39m transfer_train(__model, __batch_size, __epochs, __learning_rate)\n",
      "\u001b[1;32mc:\\Local Disk D\\M.Sc.Robotics System Engineering\\SEM-3\\FML\\BPA\\FML-WS22-BPA2_v2\\FML-WS22-BPA2_v2\\notebooks\\FML-WS22-BPA2_v2.ipynb Cell 80\u001b[0m in \u001b[0;36mtransfer_train\u001b[1;34m(model, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/M.Sc.Robotics%20System%20Engineering/SEM-3/FML/BPA/FML-WS22-BPA2_v2/FML-WS22-BPA2_v2/notebooks/FML-WS22-BPA2_v2.ipynb#Y142sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Sagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Sagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3084\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3085\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3089\u001b[0m     )\n\u001b[0;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 3, 224, 224])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(1)\n",
    "random.seed(2)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "\n",
    "__model = DropoutAdaptedVGG()\n",
    "__batch_size = 32\n",
    "__epochs = 3\n",
    "__learning_rate = 0.001\n",
    "\n",
    "\n",
    "__model.train()  # set model in training mode\n",
    "losses, valid_losses = transfer_train(__model, __batch_size, __epochs, __learning_rate)  # train the model and store loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865182c-056e-45af-9171-d8ebce9dfa5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "247ff9c07ad9f140afb363e980a19200",
     "grade": false,
     "grade_id": "cell-db0f7200dd460d46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "def plot_history():\n",
    "    epochs_axis = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs_axis, losses, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs_axis, valid_losses, \"r\", label=\"Validation loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Loss throughout training\")\n",
    "    \n",
    "plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f19e1f-f063-4d71-83bc-d5937e7fa181",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "315ada996b4eb6c5a3bad4b965d4ecaa",
     "grade": false,
     "grade_id": "cell-4392645396052633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "__Optional Question 3.6__ Implement the function `evaluate_accuracy` that computes the proportion of correctly classified data points in the test data set. The `test_loader` already hold the test data to be iterated over. You only have to implement the remaining part of the code that makes a prediction for each `input` and updates the number of correctly classified inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340c9b0-8ecd-474b-af34-95664bfb8543",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5615a963551a9329c655bf7bd0b82089",
     "grade": false,
     "grade_id": "cell-d2c0a7d65d30fbbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model: torch.nn.Module) -> float:\n",
    "    test_loader = DataLoader(test_ds)\n",
    "    n_correct = 0\n",
    "    \n",
    "    # count the number of correct classifications\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return n_correct / len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914791e3-b748-40a5-a4e0-94b356769ac5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9653be91eda094da5871d8abedbee520",
     "grade": false,
     "grade_id": "cell-ecd7ccdc9a0b9cd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Run the following cell to compute the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bfd0f-68bb-464f-b460-f5682c866b80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e7aca5ef44a9681f423fef4706660e",
     "grade": true,
     "grade_id": "cell-8a7a209e37265ff5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate_accuracy(__model)\n",
    "print(f\"Your model achieves an accuracy of {100 * accuracy:.2f}% on the test dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "942d836d3e2cd18d458d0392a068f9c8b26ded64a1e3b76273474894ee44e295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
